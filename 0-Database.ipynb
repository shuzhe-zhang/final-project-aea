{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5616d1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing standard library modules for file and text operations\n",
    "import csv\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Importing Selenium modules for web scraping\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "# Importing pandas for data manipulation and analysis\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ccb040",
   "metadata": {},
   "source": [
    "# Scrape Information From the AER Website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f25b55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_journal_info(article, driver):\n",
    "    \"\"\"\n",
    "    Extracts and returns journal name, issue, abstract, JEL codes, and article URL from a journal article webpage.\n",
    "\n",
    "    Opens the article's webpage in a new tab, scrapes the data, and then closes the tab after extraction.\n",
    "    \n",
    "    Parameters:\n",
    "        article (WebElement): The element containing the link to the article.\n",
    "        driver (WebDriver): The instance used for browser automation.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Contains (journal_name, journal_issue, abstract, jel_codes, journal_link). Elements will be None or empty if data is missing.\n",
    "        \n",
    "    Notes:\n",
    "        - Assumes a consistent HTML structure with specific CSS selectors.\n",
    "        - Errors during extraction are logged but not raised. This function does not throw exceptions but returns None values for missing data.\n",
    "    \"\"\"\n",
    "    # Save the original browser window handle\n",
    "    original_window = driver.current_window_handle\n",
    "\n",
    "    # Extract and open the article link in a new browser tab\n",
    "    paper_link = article.find_element(By.CSS_SELECTOR, \"a\").get_attribute('href')\n",
    "    driver.execute_script(\"window.open(arguments[0]);\", paper_link)\n",
    "    driver.switch_to.window(driver.window_handles[-1])\n",
    "\n",
    "    # Initialize variables to store scraped information\n",
    "    journal_name, journal_issue, abstract, jel_codes = None, None, \"\", []\n",
    "\n",
    "    try:\n",
    "        # Extract the journal name and issue from the webpage\n",
    "        journal_elements = driver.find_elements(By.CSS_SELECTOR, \"div.journal\")\n",
    "        journal_name = journal_elements[0].text if journal_elements else None\n",
    "        journal_issue = journal_elements[1].text if len(journal_elements) > 1 else None\n",
    "        \n",
    "        # Extract the abstract, stripping out JEL codes if present\n",
    "        try:\n",
    "            abstract_section = driver.find_element(By.CSS_SELECTOR, \"section.article-information.abstract\")\n",
    "            abstract_lines = abstract_section.text.splitlines()[1:]  # Skip the section title\n",
    "            abstract = \"\\n\".join(abstract_lines).strip()\n",
    "            # Remove trailing JEL code references from the abstract\n",
    "            abstract = re.sub(r'\\(JEL [A-Z][0-9]+(, [A-Z][0-9]+)*\\)\\.?$', '', abstract)\n",
    "        except Exception:\n",
    "            pass  # Ignore exceptions and leave 'abstract' empty if extraction fails\n",
    "\n",
    "        # Extract JEL codes\n",
    "        for jel in driver.find_elements(By.CSS_SELECTOR, \"ul.jel-codes > li\"):\n",
    "            try:\n",
    "                code = jel.find_element(By.CSS_SELECTOR, \"strong.code\").text\n",
    "                jel_codes.append(code)\n",
    "            except Exception:\n",
    "                pass  # Ignore exceptions and leave 'jel_codes' empty if extraction fails\n",
    "    except Exception as e:\n",
    "        # Log any errors encountered during the entire extraction process\n",
    "        print(f\"Error during scraping: {e}\")\n",
    "    finally:\n",
    "        # Close the new tab and switch back to the original window\n",
    "        driver.close()\n",
    "        driver.switch_to.window(original_window)\n",
    "\n",
    "    # Return the extracted information\n",
    "    return journal_name, journal_issue, abstract, jel_codes, paper_link\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f122b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_paper(journal_name):\n",
    "    \"\"\"\n",
    "    Extracts and compiles article details from a specified AEA journal into a CSV file.\n",
    "    \n",
    "    Iterates through all available issues of the journal on the AEA website, extracting \n",
    "    details for each article including the title, issue, journal name, abstract, \n",
    "    authors, JEL codes, and the article link, then saves these details into a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "        journal_name (str): The name of the journal to be scraped.\n",
    "\n",
    "    Returns:\n",
    "        str: The filename of the generated CSV containing the article details.\n",
    "    \"\"\"\n",
    "    # Configure Selenium browser options\n",
    "    browser_options = Options()\n",
    "    \n",
    "    # Set the option to run the browser in headless mode\n",
    "    # browser_options.add_argument(\"--headless\")\n",
    "\n",
    "    # Initialize the Chrome WebDriver with the specified options\n",
    "    driver = webdriver.Chrome(options=browser_options)\n",
    "    # Navigate to the AEA journals main page\n",
    "    driver.get(\"https://www.aeaweb.org/journals\")\n",
    "\n",
    "    # Initialize variable to hold the CSV filename\n",
    "    csv_name = None\n",
    "    \n",
    "    try:\n",
    "        # Click the journal_name link via JavaScript\n",
    "        element = driver.find_element(By.LINK_TEXT, journal_name)\n",
    "        driver.execute_script(\"arguments[0].click();\", element)\n",
    "        # Navigate to the issues section of the journal\n",
    "        driver.find_element(By.LINK_TEXT, \"Issues\").click()\n",
    "        \n",
    "        # Gather all issue links from the page\n",
    "        issue_links = driver.find_elements(By.CSS_SELECTOR, \"a[href*='issues/']\")\n",
    "        # Extract the URLs from the issue links\n",
    "        issue_urls = [link.get_attribute('href') for link in issue_links]\n",
    "        \n",
    "        # Set the filename for the CSV file to be created\n",
    "        csv_name = os.path.join(\"Raw\", f\"{journal_name.replace(': ', '-').replace(' ', '-')}.csv\")\n",
    "        # Open the CSV file for writing\n",
    "        with open(csv_name, 'w', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.writer(file)\n",
    "            # Write the CSV header row\n",
    "            writer.writerow([\"Title\", \"Issue\", \"Journal\", \"Abstract\", \"Link\"] + [f\"Author{i}\" for i in range(1, 21)] + [f\"JEL{i}\" for i in range(1, 21)])\n",
    "            \n",
    "            # Iterate through all issue URLs\n",
    "            for url in issue_urls:\n",
    "                # Open each issue in a new browser tab\n",
    "                driver.execute_script(\"window.open(arguments[0]);\", url)\n",
    "                # Switch to the new tab\n",
    "                driver.switch_to.window(driver.window_handles[-1])\n",
    "                try:\n",
    "                    # Extract details for each article in the issue\n",
    "                    for article in driver.find_elements(By.CSS_SELECTOR, \"article.journal-article\"):\n",
    "                        try:\n",
    "                            # Extract article details\n",
    "                            title = article.find_element(By.CSS_SELECTOR, \"h3.title\").text or None\n",
    "                            authors_str = article.find_element(By.CSS_SELECTOR, \"div.article-item-authors\").text\n",
    "                            authors_list = authors_str.replace(' and ', ', ').removeprefix('by ').replace('â“¡ ', ', ').split(', ')\n",
    "                            journal_name, issue, abstract, jel_codes, paper_link = scrape_journal_info(article, driver)\n",
    "                            # Prepare the data row for the CSV file\n",
    "                            row = [title, issue, journal_name, abstract, paper_link] + authors_list[:20] + [''] * (20 - len(authors_list)) + jel_codes[:20] + [''] * (20 - len(jel_codes))\n",
    "                            # Write the article details to the CSV file\n",
    "                            writer.writerow(row)\n",
    "                        except Exception:\n",
    "                            pass  # Ignore exceptions and continue with the next article\n",
    "                finally:\n",
    "                    # Close the new tab and switch back to the original window\n",
    "                    driver.close()\n",
    "                    driver.switch_to.window(driver.window_handles[0])\n",
    "    except Exception as e:\n",
    "        # Log any exceptions encountered during the process\n",
    "        print(f\"Error processing {journal_name}: {e}\")\n",
    "    finally:\n",
    "        # Quit the WebDriver and close all browser windows\n",
    "        driver.quit()\n",
    "\n",
    "    # Return the name of the created CSV file\n",
    "    return csv_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64ccacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of journal names to be scraped\n",
    "journals = [\n",
    "    \"American Economic Review\",  \n",
    "    \"AER: Insights\", \n",
    "    \"AEJ: Applied Economics\", \n",
    "    \"AEJ: Economic Policy\", \n",
    "    \"AEJ: Macroeconomics\",\n",
    "    \"AEJ: Microeconomics\" \n",
    "]\n",
    "\n",
    "# Initialize an empty list to store filenames of the CSVs created for each journal\n",
    "journals_csv = []\n",
    "\n",
    "# Iterate over each journal name\n",
    "for journal_name in journals:\n",
    "    # Call the scrape_paper function for each journal\n",
    "    csv_name = scrape_paper(journal_name)\n",
    "    # Append the resulting CSV filename to the journals_csv list\n",
    "    journals_csv.append(csv_name)\n",
    "\n",
    "# Initialize an empty DataFrame to hold the combined data from all journals\n",
    "combined_df = pd.DataFrame()\n",
    "\n",
    "# Iterate over each CSV filename\n",
    "for csv_filename in journals_csv:\n",
    "    # Read the CSV file into a temporary DataFrame\n",
    "    temp_df = pd.read_csv(csv_filename)\n",
    "    # Concatenate the temporary DataFrame with the combined DataFrame\n",
    "    combined_df = pd.concat([combined_df, temp_df], ignore_index=True)\n",
    "\n",
    "# Save the combined DataFrame to a new CSV file\n",
    "combined_df.to_csv(os.path.join('Derived', 'All-Journals.csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43c6b02",
   "metadata": {},
   "source": [
    "# Clean the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7eff0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the combined journals CSV into a DataFrame\n",
    "df = pd.read_csv(os.path.join('Derived', 'All-Journals.csv'))\n",
    "\n",
    "# Drop rows where the 'JEL1' column has missing values\n",
    "df = df.dropna(subset=['JEL1'])\n",
    "\n",
    "# Remove rows where the 'JEL1' column starts with 'Y'\n",
    "df = df[-df['JEL1'].str.startswith('Y')]\n",
    "\n",
    "# Drop columns where all values are NaN\n",
    "df = df.dropna(axis=1, how='all')\n",
    "\n",
    "# Save the cleaned DataFrame to a new CSV file\n",
    "df.to_csv(os.path.join('Derived', 'All-Journals-Cleaned.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
