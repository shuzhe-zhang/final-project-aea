{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5616d1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script is used for web scraping, particularly from https://www.aeaweb.org/journals/aer. \n",
    "It extracts article information and processes it into structured and clean CSV files.\n",
    "\n",
    "Dependencies:\n",
    "    csv: For reading and writing CSV files.\n",
    "    re: For regular expression operations, useful in text processing.\n",
    "    selenium: For automating web browser interaction.\n",
    "    pandas: For data manipulation and analysis, especially with table-like data (DataFrames).\n",
    "\"\"\"\n",
    "\n",
    "# Standard library imports for file and text operations\n",
    "import csv  # Module for reading and writing CSV files\n",
    "import re  # Module for regular expression operations\n",
    "\n",
    "# Selenium imports for web scraping\n",
    "from selenium import webdriver  # Main class for web browser automation\n",
    "from selenium.webdriver.common.by import By  # Enum for types of locating strategies in Selenium\n",
    "from selenium.webdriver.chrome.options import Options  # Class for managing options specific to Chrome WebDriver\n",
    "\n",
    "# Import for data manipulation and analysis\n",
    "import pandas as pd  # Popular data manipulation library for Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ccb040",
   "metadata": {},
   "source": [
    "## Scrape Information From the AER Website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f25b55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_journal_info(article, driver):\n",
    "    \"\"\"\n",
    "    Extracts and returns journal name, issue, abstract, JEL codes, and article URL from a journal article webpage.\n",
    "\n",
    "    Opens the article's webpage in a new tab, scrapes the data, and then closes the tab after extraction.\n",
    "    \n",
    "    Parameters:\n",
    "        article (WebElement): The element containing the link to the article.\n",
    "        driver (WebDriver): The instance used for browser automation.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Contains (journal_name, journal_issue, abstract, jel_codes, journal_link). Elements will be None or empty if data is missing.\n",
    "        \n",
    "    Notes:\n",
    "        - Assumes a consistent HTML structure with specific CSS selectors.\n",
    "        - Errors during extraction are logged but not raised. This function does not throw exceptions but returns None values for missing data.\n",
    "    \"\"\"\n",
    "    original_window = driver.current_window_handle  # Stores the handle of the original browser window.\n",
    "\n",
    "    paper_link = article.find_element(By.CSS_SELECTOR, \"a\").get_attribute('href')  # Retrieves the article's URL.\n",
    "    driver.execute_script(\"window.open(arguments[0]);\", paper_link)  # Opens the article in a new browser tab.\n",
    "    driver.switch_to.window(driver.window_handles[-1])  # Switches to the new browser tab.\n",
    "\n",
    "    # Initializes variables for data extraction.\n",
    "    journal_name, journal_issue, abstract, jel_codes = None, None, \"\", []\n",
    "\n",
    "    try:\n",
    "        # Attempts to extract the journal name and issue number.\n",
    "        journal_elements = driver.find_elements(By.CSS_SELECTOR, \"div.journal\")\n",
    "        journal_name = journal_elements[0].text if journal_elements else None\n",
    "        journal_issue = journal_elements[1].text if len(journal_elements) > 1 else None\n",
    "        \n",
    "        # Attempts to extract and format the article's abstract.\n",
    "        try:\n",
    "            abstract_section = driver.find_element(By.CSS_SELECTOR, \"section.article-information.abstract\")\n",
    "            abstract_lines = abstract_section.text.splitlines()[1:]  # Removes the title line from the abstract text.\n",
    "            abstract = \"\\n\".join(abstract_lines).strip()\n",
    "            abstract = re.sub(r'\\(JEL [A-Z][0-9]+(, [A-Z][0-9]+)*\\)\\.?$', '', abstract)  # Removes trailing JEL codes from the abstract.\n",
    "        except Exception:\n",
    "            pass  # Continues without abstract if it is not found.\n",
    "\n",
    "        # Attempts to extract JEL Classification codes.\n",
    "        for jel in driver.find_elements(By.CSS_SELECTOR, \"ul.jel-codes > li\"):\n",
    "            try:\n",
    "                code = jel.find_element(By.CSS_SELECTOR, \"strong.code\").text\n",
    "                jel_codes.append(code)\n",
    "            except Exception:\n",
    "                pass  # Skips any JEL codes that cannot be extracted.\n",
    "    except Exception as e:\n",
    "        print(f\"Error during scraping: {e}\")  # Logs any errors encountered during scraping.\n",
    "    finally:\n",
    "        driver.close()  # Closes the browser tab with the article.\n",
    "        driver.switch_to.window(original_window)  # Returns to the original browser window.\n",
    "\n",
    "    return journal_name, journal_issue, abstract, jel_codes, paper_link  # Returns the scraped data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f122b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_paper(journal_name):\n",
    "    \"\"\"\n",
    "    Extracts and compiles article details from a specified AEA journal into a CSV file.\n",
    "    \n",
    "    Iterates through all available issues of the journal on the AEA website, extracting \n",
    "    details for each article including the title, issue, journal name, abstract, \n",
    "    authors, JEL codes, and the article link, then saves these details into a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "        journal_name (str): The name of the journal to be scraped.\n",
    "\n",
    "    Returns:\n",
    "        str: The filename of the generated CSV containing the article details.\n",
    "    \"\"\"\n",
    "    browser_options = Options()  # Initialize browser settings for WebDriver.\n",
    "    driver = webdriver.Chrome(options=browser_options)  # Create a new instance of the Chrome WebDriver.\n",
    "    driver.get(\"https://www.aeaweb.org/journals\")  # Navigate to the AEA journals listing page.\n",
    "\n",
    "    try:\n",
    "        # Navigate to the specific journal's page and access its issues section.\n",
    "        driver.find_element(By.LINK_TEXT, journal_name).click()\n",
    "        driver.find_element(By.LINK_TEXT, \"Issues\").click()\n",
    "        \n",
    "        # Extract all issue links available on the journal's page.\n",
    "        issue_links = driver.find_elements(By.CSS_SELECTOR, \"a[href*='issues/']\")\n",
    "        issue_urls = [link.get_attribute('href') for link in issue_links]\n",
    "        \n",
    "        # Set up the CSV file that will store the extracted article information.\n",
    "        csv_name = f\"{journal_name.replace(': ', '-').replace(' ', '-')}.csv\"\n",
    "        with open(csv_name, 'w', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.writer(file)\n",
    "            # Define and write the header row of the CSV file.\n",
    "            writer.writerow([\"Title\", \"Issue\", \"Journal\", \"Abstract\", \"Link\"] + [f\"Author{i}\" for i in range(1, 21)] + [f\"JEL{i}\" for i in range(1, 21)])\n",
    "            \n",
    "            # Iterate through each issue URL and extract article details.\n",
    "            for url in issue_urls:\n",
    "                driver.execute_script(\"window.open(arguments[0]);\", url)  # Open each issue in a new browser tab.\n",
    "                driver.switch_to.window(driver.window_handles[-1])  # Switch context to the new tab.\n",
    "                try:\n",
    "                    # Iterate through each article in the issue and extract relevant details.\n",
    "                    for article in driver.find_elements(By.CSS_SELECTOR, \"article.journal-article\"):\n",
    "                        try:\n",
    "                            # Extract individual article details and compile them into a CSV row.\n",
    "                            title = article.find_element(By.CSS_SELECTOR, \"h3.title\").text or None\n",
    "                            authors_str = article.find_element(By.CSS_SELECTOR, \"div.article-item-authors\").text\n",
    "                            authors_list = authors_str.replace(' and ', ', ').strip('by ').split(', ')  # Clean and split author names.\n",
    "                            journal_name, issue, abstract, jel_codes, paper_link = scrape_journal_info(article, driver)\n",
    "                            # Combine all article details and append to the CSV.\n",
    "                            row = [title, issue, journal_name, abstract, paper_link] + authors_list[:20] + [''] * (20 - len(authors_list)) + jel_codes[:20] + [''] * (20 - len(jel_codes))\n",
    "                            writer.writerow(row)\n",
    "                        except Exception:\n",
    "                            pass  # Continue to the next article if there's an error.\n",
    "                finally:\n",
    "                    driver.close()  # Close the current tab with the issue.\n",
    "                    driver.switch_to.window(driver.window_handles[0])  # Return to the main journal page tab.\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {journal_name}: {e}\")  # Log any encountered errors.\n",
    "    finally:\n",
    "        driver.quit()  # Close the browser once all processing is complete.\n",
    "\n",
    "    return csv_name  # Return the path of the filled CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c64ccacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of target journals to scrape from the AEA website.\n",
    "journals = [\n",
    "    \"American Economic Review\",  \n",
    "    \"AER: Insights\", \n",
    "    \"AEJ: Applied Economics\", \n",
    "    \"AEJ: Economic Policy\", \n",
    "    \"AEJ: Macroeconomics\",\n",
    "    \"AEJ: Microeconomics\" \n",
    "]\n",
    "\n",
    "# Initialize an empty list to store CSV filenames for each scraped journal.\n",
    "journals_csv = []\n",
    "\n",
    "# Iterate through each journal, scrape its data, and save the filename of the created CSV.\n",
    "for journal_name in journals:\n",
    "    csv_name = scrape_paper(journal_name)  # Call scraping function.\n",
    "    journals_csv.append(csv_name)  # Append the CSV filename to the list.\n",
    "    \n",
    "# Initialize an empty DataFrame to combine all scraped data.\n",
    "combined_df = pd.DataFrame()\n",
    "\n",
    "# Iterate through each generated CSV file, read its contents, and combine it into one DataFrame.\n",
    "for csv_filename in journals_csv:\n",
    "    temp_df = pd.read_csv(csv_filename)  # Read individual journal CSV into a temporary DataFrame.\n",
    "    combined_df = pd.concat([combined_df, temp_df], ignore_index=True)  # Combine with the main DataFrame.\n",
    "\n",
    "# Export the combined DataFrame containing all journals' data into a single CSV file.\n",
    "combined_df.to_csv('All-Journals.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43c6b02",
   "metadata": {},
   "source": [
    "## Clean the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "db7eff0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('All-Journals.csv')\n",
    "\n",
    "# Filter out rows where the first JEL code starts with 'Y'\n",
    "df = df.dropna(subset=['JEL1'])\n",
    "df = df[-df['JEL1'].str.startswith('Y')] # Use '-' for negation\n",
    "\n",
    "# Drop columns where all values are NaN\n",
    "df = df.dropna(axis=1, how='all')\n",
    "\n",
    "# Save the cleaned DataFrame to a new CSV file, without the index\n",
    "df.to_csv('All-Journals-Cleaned.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
