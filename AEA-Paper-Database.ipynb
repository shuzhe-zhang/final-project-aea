{
 "cells": [
  {
   "cell_type": "raw",
   "id": "0c50b25a",
   "metadata": {},
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5616d1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import time\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f25b55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_journal_info(article, driver):\n",
    "    \"\"\"\n",
    "    Extracts information from a journal article's webpage.\n",
    "\n",
    "    Navigates to an article's page and extracts the journal name, issue, abstract,\n",
    "    and JEL codes using the provided Selenium WebDriver.\n",
    "\n",
    "    Parameters:\n",
    "        article (WebElement): The element containing the article's link.\n",
    "        driver (WebDriver): Selenium WebDriver for browser automation.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Contains journal_name, journal_issue, abstract, jel_codes.\n",
    "    \"\"\"\n",
    "    original_window = driver.current_window_handle  # Remember the initial browser window\n",
    "\n",
    "    paper_link = article.find_element(By.CSS_SELECTOR, \"a\").get_attribute('href')  # Get article's URL\n",
    "    driver.execute_script(\"window.open(arguments[0]);\", paper_link)  # Open the article in a new tab\n",
    "    driver.switch_to.window(driver.window_handles[-1])  # Switch to the new tab\n",
    "\n",
    "    journal_name, journal_issue, abstract, jel_codes = None, None, \"\", []  # Initialize empty data variables\n",
    "\n",
    "    try:\n",
    "        # Extract journal name and issue\n",
    "        journal_elements = driver.find_elements(By.CSS_SELECTOR, \"div.journal\")\n",
    "        journal_name = journal_elements[0].text if journal_elements else None\n",
    "        journal_issue = journal_elements[1].text if len(journal_elements) > 1 else None\n",
    "        \n",
    "        # Extract and clean article abstract\n",
    "        try:\n",
    "            abstract_section = driver.find_element(By.CSS_SELECTOR, \"section.article-information.abstract\")\n",
    "            abstract_lines = abstract_section.text.splitlines()[1:]  # Remove title line from abstract\n",
    "            abstract = \"\\n\".join(abstract_lines).strip()\n",
    "            abstract = re.sub(r'\\(JEL [A-Z][0-9]+(, [A-Z][0-9]+)*\\)\\.?$', '', abstract)  # Strip trailing JEL codes\n",
    "        except Exception:\n",
    "            pass  # Proceed without abstract if not found\n",
    "\n",
    "        # Extract JEL Classification codes\n",
    "        for jel in driver.find_elements(By.CSS_SELECTOR, \"ul.jel-codes > li\"):\n",
    "            try:\n",
    "                code = jel.find_element(By.CSS_SELECTOR, \"strong.code\").text\n",
    "                jel_codes.append(code)\n",
    "            except Exception:\n",
    "                pass  # Skip any JEL code extraction issues\n",
    "    except Exception as e:\n",
    "        print(f\"Error during scraping: {e}\")  # Log general scraping errors\n",
    "    finally:\n",
    "        driver.close()  # Close the article tab\n",
    "        driver.switch_to.window(original_window)  # Return to the original browser window\n",
    "\n",
    "    return journal_name, journal_issue, abstract, jel_codes  # Return collected data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f122b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_paper(journal_name):\n",
    "    \"\"\"\n",
    "    Scrapes articles from a specified journal listed on the AEA website.\n",
    "\n",
    "    Navigates to the journal's webpage, iterates through all available issues,\n",
    "    and extracts details for each article. Details include the title, issue,\n",
    "    journal name, abstract, authors, and JEL codes. This information is then\n",
    "    saved to a CSV file named after the journal.\n",
    "\n",
    "    Parameters:\n",
    "        journal_name (str): The name of the journal to scrape.\n",
    "\n",
    "    Returns:\n",
    "        str: The name of the created CSV file containing the scraped data.\n",
    "    \"\"\"\n",
    "    browser_options = Options()\n",
    "    driver = webdriver.Chrome(options=browser_options)\n",
    "    driver.get(\"https://www.aeaweb.org/journals\")\n",
    "\n",
    "    try:\n",
    "        driver.find_element(By.LINK_TEXT, journal_name).click()\n",
    "        driver.find_element(By.LINK_TEXT, \"Issues\").click()\n",
    "\n",
    "        issue_links = driver.find_elements(By.CSS_SELECTOR, \"a[href*='issues/']\")\n",
    "        issue_urls = [link.get_attribute('href') for link in issue_links]\n",
    "        csv_name = f\"{journal_name.replace(': ', '-').replace(' ', '-')}.csv\"\n",
    "\n",
    "        with open(csv_name, 'w', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.writer(file)\n",
    "            header_row = [\"Title\", \"Issue\", \"Journal\", \"Abstract\"] + \\\n",
    "                         [f\"Author{i}\" for i in range(1, 21)] + [f\"JEL{i}\" for i in range(1, 21)]\n",
    "            writer.writerow(header_row)\n",
    "\n",
    "            for url in issue_urls:\n",
    "                driver.execute_script(\"window.open(arguments[0]);\", url)\n",
    "                driver.switch_to.window(driver.window_handles[-1])\n",
    "\n",
    "                try:\n",
    "                    article_links = driver.find_elements(By.CSS_SELECTOR, \"article.journal-article\")\n",
    "                    for article in article_links:\n",
    "                        title, authors_list = None, []\n",
    "\n",
    "                        try:\n",
    "                            title = article.find_element(By.CSS_SELECTOR, \"h3.title\").text\n",
    "                        except Exception:\n",
    "                            pass\n",
    "                        \n",
    "                        try:\n",
    "                            authors_str = article.find_element(By.CSS_SELECTOR, \"div.article-item-authors\").text\n",
    "                            authors_str = authors_str[3:] if authors_str.lower().startswith('by ') else authors_str\n",
    "                            authors_str = authors_str.replace(' and ', ', ')\n",
    "                            authors_list = authors_str.split(', ')\n",
    "                        except Exception:\n",
    "                            pass\n",
    "                        \n",
    "                        try:\n",
    "                            _, journal_details, cleaned_abstract, jel_codes = scrape_journal_info(article, driver)\n",
    "                        except Exception:\n",
    "                            pass\n",
    "                        \n",
    "                        row = [title, journal_details, journal_name, cleaned_abstract] + \\\n",
    "                              authors_list[:20] + [''] * (20 - len(authors_list)) + \\\n",
    "                              jel_codes[:20] + [''] * (20 - len(jel_codes))\n",
    "                        writer.writerow(row)\n",
    "\n",
    "                finally:\n",
    "                    driver.close()\n",
    "                    driver.switch_to.window(driver.window_handles[0])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing {journal_name}: {str(e)}\")\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "    return csv_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64ccacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "journals = [\n",
    "    \"American Economic Review\",  \n",
    "    \"AER: Insights\", \n",
    "    \"AEJ: Applied Economics\", \n",
    "    \"AEJ: Economic Policy\", \n",
    "    \"AEJ: Macroeconomics\",\n",
    "    \"AEJ: Microeconomics\" \n",
    "]\n",
    "\n",
    "journals_csv = []\n",
    "\n",
    "for journal_name in journals:\n",
    "    csv_name = scrape_paper(journal_name) \n",
    "    journals_csv.append(csv_name)\n",
    "    \n",
    "combined_df = pd.DataFrame()\n",
    "\n",
    "for csv_filename in journals_csv:\n",
    "    temp_df = pd.read_csv(csv_filename)\n",
    "    combined_df = pd.concat([combined_df, temp_df], ignore_index=True)\n",
    "\n",
    "combined_df.to_csv('All-Journals.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26403f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_transform_aej_data(input_filename):\n",
    "    \"\"\"\n",
    "    Transforms AEJ article data from wide to long format and cleans it.\n",
    "\n",
    "    Specific operations:\n",
    "    - Removes entries without an abstract.\n",
    "    - Assigns sequential Paper_IDs.\n",
    "    - Consolidates author and JEL columns into semicolon-separated strings.\n",
    "    - Reshapes data to long format for both authors and JEL codes.\n",
    "    - Sorts data and outputs to a new CSV, suffixing the original filename with '-Cleaned'.\n",
    "\n",
    "    Parameters:\n",
    "        input_filename (str): Path to the source CSV file.\n",
    "\n",
    "    Output:\n",
    "        None, but writes transformed data to '{original_filename}-Cleaned.csv'.\n",
    "    \"\"\"\n",
    "    # Setup new file name and read data\n",
    "    output_filename = input_filename.replace('.csv', '-Cleaned.csv')\n",
    "    df = pd.read_csv(input_filename)\n",
    "    \n",
    "    # Clean and transform data\n",
    "    df.dropna(subset=['Abstract'], inplace=True)\n",
    "    df['Paper_ID'] = range(1, len(df) + 1)\n",
    "    df['Authors Combined'] = df[[f'Author{i}' for i in range(1, 21)]].apply(lambda x: '; '.join(x.dropna()), axis=1)\n",
    "    df['JEL Combined'] = df[[f'JEL{i}' for i in range(1, 21)]].apply(lambda x: '; '.join(x.dropna()), axis=1)\n",
    "\n",
    "    # Reshape data into long format\n",
    "    long_df_authors = pd.melt(df, id_vars=['Paper_ID', 'Title', 'Issue', 'Journal', 'Abstract', 'Authors Combined', 'JEL Combined'],\n",
    "                              var_name='Author_ID', value_name='Author').dropna(subset=['Author'])\n",
    "    long_df_authors['Author_ID'] = long_df_authors['Author_ID'].str.extract('(\\d+)').astype(int)\n",
    "\n",
    "    long_df_jel = pd.melt(long_df_authors, id_vars=['Paper_ID', 'Title', 'Issue', 'Journal', 'Abstract', 'Authors Combined', 'Author_ID', 'Author'],\n",
    "                          var_name='JEL_ID', value_name='JEL Code').dropna(subset=['JEL Code'])\n",
    "    long_df_jel['JEL_ID'] = long_df_jel['JEL_ID'].str.extract('(\\d+)').astype(int)\n",
    "\n",
    "    # Sort and save cleaned data\n",
    "    long_df_jel.sort_values(by=['Paper_ID', 'Author_ID', 'JEL_ID'], ascending=[True, True, True], inplace=True)\n",
    "    long_df_jel.to_csv(output_filename, index=False)\n",
    "\n",
    "# Run the function for a specified file\n",
    "clean_and_transform_aej_data('All-Journals.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
